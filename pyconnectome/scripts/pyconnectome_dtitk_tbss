#!/usr/bin/env python
##########################################################################
# NSAp - Copyright (C) CEA, 2018
# Distributed under the terms of the CeCILL-B license, as published by
# the CEA-CNRS-INRIA. Refer to the LICENSE file or to
# http://www.cecill.info/licences/Licence_CeCILL-B_V1-en.html
# for details.
##########################################################################

# System import
import os
import argparse
import textwrap
import glob
import re
import json
import shutil
import progressbar
from datetime import datetime
from argparse import RawTextHelpFormatter
from datetime import datetime
from pprint import pprint

# Bredala import
try:
    import bredala
    bredala.USE_PROFILER = False
    bredala.register("pyconnectome.tractography.dtitk_tbss",
                     names=["bootstrap_template_from_dti", "generate_FA_map",
                            "skeletonize", "fslmerge", "get_fa_stack_mask",
                            "tbss_4_prestats", "dtitk_version", "get_mean_fa"])
except:
    pass

# DTI-TK/TBSS imports
from pyconnectome import DEFAULT_FSL_PATH
from pyconnectome.tractography.dtitk_tbss import dtitk_version
from pyconnectome.tractography.dtitk_tbss import (
    bootstrap_template_from_dti, generate_FA_map, skeletonize, fslmerge,
    get_fa_stack_mask, tbss_4_prestats, get_mean_fa)

# Pyconnectome imports
import pyconnectome

# Script documentation
DOC = """
Generate FA map with DTI-TK/TBSS commands.
------------------------------------------

Steps:
    1) Generate population-specific DTI template with the isotropic 1mm3
    spacing.
    2) Generate the FA map from template.
    3) Generate the white matter skeleton from the FA map.
    4) Generate the subjects concatenated FA maps.
    5) Project all subjects' FA data onto the mean FA skeleton.

Command example on the MAPT data :
python $HOME/git/pyconnectome/pyconnectome/scripts/pyconnectome_dtitk_tbss \
    -t 03990399MRO/dtifit_dtitk_diffeo.nii.gz \
        ...
       03990185BAI/dtifit_dtitk_diffeo.nii.gz \
       03990230CRE/dtifit_dtitk_diffeo.nii.gz \
       03990364BCL/dtifit_dtitk_diffeo.nii.gz \
       02990271GJO/dtifit_dtitk_diffeo.nii.gz \
    -o /volatile/MAPT_TRACTS/data/M0 \
    -V 2
"""


def is_file(filepath):
    """ Check file's existence - argparse 'type' argument.
    """
    if not os.path.isfile(filepath):
        raise argparse.ArgumentError("File does not exist: %s" % filepath)
    return filepath


def is_directory(dirarg):
    """ Type for argparse - checks that directory exists.
    """
    if not os.path.isdir(dirarg):
        raise argparse.ArgumentError(
            "The directory '{0}' does not exist!".format(dirarg))
    return dirarg


# Parse input arguments
def get_cmd_line_args():
    """
    Create a command line argument parser and return a dict mapping
    <argument name> -> <argument value>.
    """
    parser = argparse.ArgumentParser(
        prog="python pyconnectome_dtitk_tbss",
        description=textwrap.dedent(DOC),
        formatter_class=RawTextHelpFormatter)

    # Required arguments
    required = parser.add_argument_group("required arguments")
    required.add_argument(
        "-t", "--tensor-normalized", type=is_file, required=True, nargs="+",
        help="Paths to the tensors in template space and with 1mm3 isotropic"
             " voxel dimensions.")
    required.add_argument(
        "-p", "--sid-position",
        type=int, required=True,
        help="Path to the output directoryThe subject identifier position in "
             "the tensor path.")
    required.add_argument(
        "-o", "--outdir",
        type=is_directory, required=True, metavar="<path>",
        help="Path to the output directory.")

    # Optional argument
    parser.add_argument(
        "-T", "--template", type=is_file,
        help="Directly give template to generate FA skeleton. If set to None,"
             " template of isotropic dimensions will be created by"
             " bootstrapping all the subjects tensor files.")
    parser.add_argument(
        "-F", "--template-fa", type=is_file,
        help="Template FA map.")
    parser.add_argument(
        "-E", "--subjects-in-error", type=is_file,
        help="The list of subjects in error (ie. that will not be "
             "considered).")
    parser.add_argument(
        "-S", "--template-fa-skeleton", type=is_file,
        help="Template FA skeleton.")
    parser.add_argument(
        "-K", "--threshold-fa-skeleton", type=float, default=0.2,
        help="Fa skeleton threshold for tbss_4_prestats.")
    parser.add_argument(
        "-C", "--fsl-config", metavar="<path>", type=is_file,
        help="Path to fsl sh config file.")
    parser.add_argument(
        "-V", "--verbose",
        type=int, choices=[0, 1, 2], default=2,
        help="Increase the verbosity level: 0 silent, [1, 2] verbose.")

    # Create a dict of arguments to pass to the 'main' function
    args = parser.parse_args()
    kwargs = vars(args)
    if kwargs["fsl_config"] is None:
        kwargs["fsl_config"] = DEFAULT_FSL_PATH
    verbose = kwargs.pop("verbose")
    return kwargs, verbose


"""
Parse the command line.
"""
inputs, verbose = get_cmd_line_args()
runtime = {
    "timestamp": datetime.now().isoformat(),
    "tool": "pyconnectome_dtitk_tbss",
    "tool_version": pyconnectome.__version__,
    "dtitk_version": dtitk_version()
}
outputs = {}
if verbose > 0:
    pprint("[info] Starting tbss with enigma template...")
    pprint("[info] Runtime:")
    pprint(runtime)
    pprint("[info] Inputs:")
    pprint(inputs)


"""
1 - If necessary, generate the population-specific DTI template.
"""
if inputs["template"] is None:
    print("Generate the population-specific DTI template with the isotropic"
          " 1mm3 spacing...")
    # List normalized dti files
    subjects_normalized_file = os.path.join(
        inputs["outdir"], "subjects_normalized.txt")
    with open(subjects_normalized_file, 'w') as open_file:
        for t in inputs["tensor_normalized"]:
            open_file.write(t)
            open_file.write("\n")
    template = os.path.join(
        inputs["outdir"], "mean_final_high_res.nii.gz")
    bootstrap_template_from_dti(
        subjects=subjects_normalized_file,
        out_template=template,
        typep="ORIGINAL",
        interp="LEI")
else:
    template = inputs["template"]
outputs["template"] = template


"""
2 - Generate the FA map of the high-resolution population-specific DTI
    template.
"""
if inputs["template_fa"] is None:
    print("Generate template FA map...")
    template_fa = os.path.join(inputs["outdir"], "mean_FA.nii.gz")
    generate_FA_map(
        dti_file=template,
        output_fa=template_fa)
else:
    template_fa = inputs["template_fa"]
outputs["template_fa"] = template_fa


"""
3 - Generate the white matter skeleton from the high-resolution FA map of
    the DTI template.
"""
if inputs["template_fa_skeleton"] is None:
    print("Generate skeleton from template FA map...")
    template_fa_skeleton = os.path.join(
        inputs["outdir"], "mean_FA_skeleton.nii.gz")
    skeletonize(
        input_file=template_fa,
        output_file=template_fa_skeleton,
        fsl_sh=inputs["fsl_config"])
else:
    template_fa_skeleton = inputs["template_fa_skeleton"]
outputs["mean_FA_skeleton"] = template_fa_skeleton


"""
4 - Generate the FA maps of the spatially normalized high-resolution DTI
    data.
"""
print("Generate FA maps of DTI data...")
subjects_fa_map = []
with open(inputs["subjects_in_error"], "rt") as open_file:
    subjects_in_error = [row.rstrip("\n") for row in open_file.readlines()]
nb_files = len(inputs["tensor_normalized"])
subjects = []
with progressbar.ProgressBar(max_value=nb_files) as bar:
    for cnt, dti_file in enumerate(inputs["tensor_normalized"]):
        sub_fa_map = dti_file.replace(".nii.gz", "_FA.nii.gz")
        if not os.path.isfile(sub_fa_map):
            sub_fa_map = generate_FA_map(
                dti_file=dti_file,
                output_fa=sub_fa_map)
        sid = dti_file.split(os.sep)[inputs["sid_position"]]
        if sid in subjects_in_error:
            print("Skipping {0}.".format(sid))
            continue
        subjects.append(sid)
        subjects_fa_map.append(sub_fa_map)
        bar.update(cnt + 1)
subjects_file = os.path.join(inputs["outdir"], "subjects.txt")
with open(subjects_file, "wt") as open_file:
    open_file.write("\n".join(subjects))
fa_stack = os.path.join(inputs["outdir"], "all_FA.nii.gz")
if not os.path.isfile(fa_stack):
    fa_stack = fslmerge(
        images=subjects_fa_map,
        concatenated_output=fa_stack,
        time=True,
        fsl_sh=inputs["fsl_config"])
outputs["all_FA"] = fa_stack
mean_fa = os.path.join(inputs["outdir"], "mean_FA.nii.gz")
if not os.path.isfile(mean_fa):
    get_mean_fa(
        fa_4D=fa_stack,
        output=mean_fa,
        fsl_sh=inputs["fsl_config"])
mean_fa_mask = os.path.join(inputs["outdir"], "mean_FA_mask.nii.gz")
if not os.path.isfile(mean_fa_mask):
    get_fa_stack_mask(
        fa_4D=fa_stack,
        output=mean_fa_mask,
        fsl_sh=inputs["fsl_config"])
outputs["mean_FA_mask"] = mean_fa_mask


"""
5 - Process to tbss procedure with tbss_4_prestats and project all subjects'
    FA data onto the mean FA skeleton.
"""
# Place the TBSS relevant files into a folder that TBSS expects
# Create a directory called tbss with a subdirectory called stats.
# Copy mean_FA_skeleton, all_FA, and mean_FA_mask to the stats
# subdirectory.
tbss_dir = os.path.join(inputs["outdir"], "tbss")
tbss_stat_dir = os.path.join(tbss_dir, "stats")
if not os.path.isdir(tbss_stat_dir):
    os.makedirs(tbss_stat_dir)
tbss_stat_files = [
    ("mean_FA_skeleton", template_fa_skeleton),
    ("all_FA", fa_stack),
    ("mean_FA_mask", mean_fa_mask),
    ("mean_FA", mean_fa)]
for name, path in tbss_stat_files:
    shutil.copy2(path, os.path.join(tbss_stat_dir, name + ".nii.gz"))
outputs["tbss_dir"] = tbss_dir
outputs["tbss_stats_dir"] = tbss_stat_dir

# Thresholds the mean FA skeleton image at the chosen threshold with
# tbss_4_prestats
tbss_4_prestats(
    tbss_dir=tbss_dir,
    threshold=inputs["threshold_fa_skeleton"],
    fsl_sh=inputs["fsl_config"])


"""
Update the outputs and save them and the inputs in a 'logs' directory.
"""
logdir = os.path.join(inputs["outdir"], "logs")
if not os.path.isdir(logdir):
    os.mkdir(logdir)
for name, final_struct in [("inputs", inputs), ("outputs", outputs),
                           ("runtime", runtime)]:
    log_file = os.path.join(
        logdir, "pyconnectome_dtitk_tbss_{0}.json".format(name))
    with open(log_file, "wt") as open_file:
        json.dump(final_struct, open_file, sort_keys=True, check_circular=True,
                  indent=4)
if verbose > 0:
    pprint("[Info] Outputs:")
    pprint(outputs)
