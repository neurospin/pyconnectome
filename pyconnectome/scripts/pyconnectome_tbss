#!/usr/bin/env python
##########################################################################
# NSAp - Copyright (C) CEA, 2018
# Distributed under the terms of the CeCILL-B license, as published by
# the CEA-CNRS-INRIA. Refer to the LICENSE file or to
# http://www.cecill.info/licences/Licence_CeCILL-B_V1-en.html
# for details.
##########################################################################

# System import
import os
import argparse
import textwrap
import glob
import re
import json
import shutil
from datetime import datetime
from argparse import RawTextHelpFormatter
from datetime import datetime
from pprint import pprint

# Bredala import
try:
    import bredala
    bredala.USE_PROFILER = False
    bredala.register("pyconnectome.tractography.dtitk_tbss",
                     names=["tbss_1_preproc", "tbss_2_reg", "tbss_3_postreg",
                            "tbss_4_prestats"])
except:
    pass

# TBSS import
from pyconnectome.tractography.dtitk_tbss import (
    tbss_1_preproc, tbss_2_reg, tbss_3_postreg, tbss_4_prestats)

# Pyconnectome imports
import pyconnectome
from pyconnectome.wrapper import FSLWrapper
from pyconnectome import DEFAULT_FSL_PATH

# Script documentation
DOC = """
TBSS pipeline analysis.
-----------------------

Run TBSS pipeline to generate subjects' FA skeletons in a single 4D file that
can be used for voxelwise statistics.

To generate the statistics, first create the design and the contrast matrices.
Note that the order of the entries (rows) in your design matrix must match the
alphabetical order of your original FA images, as that determines the order of
the aligned FA images in the final 4D file all_FA_skeletonised; check this with:

cd FA
imglob *_FA.*

For the design matix, one column for each EV, and one row for each subject.

1 0
1 0
0 1

All you need to do is save this data as design.txt, then run:

Text2Vest design.txt design.mat

For the contrasts, one row for each contrast, and column for each EV. For
example, we have four EVs in our design matrix above. A contrast matrix for
this design might look like this (tw0 contrasts, each giving the mean
activation for one of the groups in the study):

1 0
0 1

If you save this as contrasts.txt, simply run this:

Text2Vest contrasts.txt design.con

For the statistics, simply run this:

randomise -i all_FA_skeletonised -o tbss -m mean_FA_skeleton_mask -d design.mat -t design.con -n 500 --T2 -D
or
randomise_parallel with SGE

fslview $FSLDIR/data/standard/MNI152_T1_1mm mean_FA_skeleton -l Green -b 0.2,0.8 tbss_tstat1 -l Red-Yellow -b 3,6 tbss_tstat2 -l Blue-Lightblue -b 3,6

Command example on the MAPT data :

python $HOME/git/pyconnectome/pyconnectome/scripts/pyconnectome_tbss \
    -s 02990191LSI 02990236RMO 02990247CCO \
    -f 02990191LSI/4-Tensor/dtifit/dtifit_FA.nii.gz \
       02990236RMO/4-Tensor/dtifit/dtifit_FA.nii.gz \
       02990247CCO/4-Tensor/dtifit/dtifit_FA.nii.gz \
    -o /volatile/test_enigma_tbss_pipeline \
    -U \
    -A \
    -P \
    -R \
    -G \
    -S \
    -F /neurospin/nsap/local/fsl-5.0.11.sh \
    -V 2

Command example on SENIOR:

python $HOME/git/pyconnectome/pyconnectome/scripts/pyconnectome_tbss \
    -s aa140422 ab100207 ab120348 \
    -f /neurospin/senior/nsap/data/V0/connectomist/aa140422/dtifit/dti_fa.nii.gz \
       /neurospin/senior/nsap/data/V0/connectomist/ab100207/dtifit/dti_fa.nii.gz \
       /neurospin/senior/nsap/data/V0/connectomist/ab120348/dtifit/dti_fa.nii.gz \
    -o /neurospin/nsap/research/tbss/senior \
    -S \
    -U \
    -M \
    -F /neurospin/nsap/local/fsl-5.0.11.sh \
    -V 2
"""


def is_file(filepath):
    """ Check file's existence - argparse 'type' argument.
    """
    if not os.path.isfile(filepath):
        raise argparse.ArgumentError("File does not exist: %s" % filepath)
    return filepath


def is_directory(dirarg):
    """ Type for argparse - checks that directory exists.
    """
    if not os.path.isdir(dirarg):
        raise argparse.ArgumentError(
            "The directory '{0}' does not exist!".format(dirarg))
    return dirarg


# Parse input arguments
def get_cmd_line_args():
    """
    Create a command line argument parser and return a dict mapping
    <argument name> -> <argument value>.
    """
    parser = argparse.ArgumentParser(
        prog="python pyconnectome_tbss",
        description=textwrap.dedent(DOC),
        formatter_class=RawTextHelpFormatter)

    # Required arguments
    required = parser.add_argument_group("required arguments")
    required.add_argument(
        "-s", "--subjects", type=is_file, required=True,
        help="Subjects ids: one line per subject.")
    required.add_argument(
        "-f", "--fa-files", type=is_file, required=True,
        help="Subjects fa files in the same order than the subjects ids: one "
             "line per subject.")
    required.add_argument(
        "-o", "--outdir",
        type=is_directory, required=True, metavar="<path>",
        help="Path to the output directory.")

    # Optional argument
    parser.add_argument(
        "-C", "--clean", action="store_true",
        help="Delete existing files in output directory.")
    parser.add_argument(
        "-N", "--init", action="store_true",
        help="Copy FA files into the tbss directory.")
    parser.add_argument(
        "-A", "--rename-fa", action="store_true",
        help="Rename FA files during the init copy step using the subjects' "
             "ids.")
    parser.add_argument(
        "-P", "--preproc", action="store_true",
        help="Run TBSS first step: preprocessing.")
    parser.add_argument(
        "-R", "--registration", action="store_true",
        help="Run TBSS second step: subjects registration.")
    parser.add_argument(
        "-G", "--postreg", action="store_true",
        help="Run TBSS third step: post registration.")
    parser.add_argument(
        "-S", "--pre-stats", action="store_true",
        help="Run TBSS fourth step: prestats.")
    parser.add_argument(
        "-B", "--find-best-target", action="store_true",
        help="Use best target image for TBSS registration.")
    parser.add_argument(
        "-U", "--use-fmrib58-fa", action="store_true",
        help="Use fmrib58 template for TBSS registration.")
    parser.add_argument(
        "-I", "--use-target-img", type=is_file, metavar="<path>",
        help="Use target image for TBSS registration.")
    parser.add_argument(
        "-M", "--use-fmrib58-fa-mean-and-skel", action="store_true",
        help="use the FMRIB58 mean FA image and its derived skeleton, "
             "instead of the mean of the subjects.")
    parser.add_argument(
        "-T", "--threshold-fa-skeleton", type=float, default=0.2,
        help="Threshold for the mean FA skeleton.")
    parser.add_argument(
        "-F", "--fsl-config", metavar="<path>", type=is_file,
        help="Path to fsl sh config file.")
    parser.add_argument(
        "-V", "--verbose",
        type=int, choices=[0, 1, 2], default=2,
        help="Increase the verbosity level: 0 silent, [1, 2] verbose.")

    # Create a dict of arguments to pass to the 'main' function
    args = parser.parse_args()
    kwargs = vars(args)
    verbose = kwargs.pop("verbose")
    if kwargs["fsl_config"] is None:
        kwargs["fsl_config"] = DEFAULT_FSL_PATH
    return kwargs, verbose


"""
Parse the command line.
"""

inputs, verbose = get_cmd_line_args()
runtime = {
    "timestamp": datetime.now().isoformat(),
    "tool": "pyconnectome_tbss",
    "tool_version": pyconnectome.__version__,
    "fsl_version": FSLWrapper([], shfile=inputs["fsl_config"]).version
}
outputs = {}
if verbose > 0:
    pprint("[info] Starting TBSS analysis...")
    pprint("[info] Runtime:")
    pprint(runtime)
    pprint("[info] Inputs:")
    pprint(inputs)


"""
Check/load input parameters
"""
if (inputs["registration"] and not inputs["find_best_target"]
        and not inputs["use_fmrib58_fa"] and inputs["use_target_img"] is None):
    raise ValueError(
        "Please enter at least one parameter, -B, -F, -I for TBSS "
        "registration.")
tbss_dir = os.path.join(inputs["outdir"], "tbss")
if not os.path.isdir(tbss_dir):
    os.mkdir(tbss_dir)
outputs["tbss_dir"] = tbss_dir
with open(inputs["subjects"], "rt") as open_file:
    subjects = [row.rstrip("\n") for row in open_file.readlines()]
with open(inputs["fa_files"], "rt") as open_file:
    fa_files = [row.rstrip("\n") for row in open_file.readlines()]


"""
Pre-Step: copy/renaming
"""

if inputs["init"]:
    print("Copying FA files in tbss directory...")
    for sid, fa_file in zip(subjects, fa_files):
        basename = os.path.basename(fa_file)
        if inputs["rename_fa"]:
            basename = sid + "_" + basename
        out_fa = os.path.join(tbss_dir, basename)
        if not os.path.isfile(out_fa):
            shutil.copy2(fa_file, out_fa)


"""
Step 1: preproc
"""

if inputs["preproc"]:
    print("Preprocessing FA files...")
    fa_dir, orig_dir = tbss_1_preproc(
        tbss_dir=tbss_dir,
        fsl_sh=inputs["fsl_config"])
    outputs["fa_dir"] = fa_dir
    outputs["orig_dir"] = orig_dir


"""
Step 2: registration
"""

if inputs["registration"]:
    print("Registering FA files to template...")
    tbss_2_reg(
        tbss_dir=tbss_dir,
        use_fmrib58_fa_1mm=inputs["use_fmrib58_fa"],
        target_img=inputs["use_target_img"],
        find_best_target=inputs["find_best_target"],
        fsl_sh=inputs["fsl_config"])


"""
Step 3: apply registration and create skeleton
"""

if inputs["postreg"]:
    print("Applying transformation FA files and creating mean FA...")
    all_fa, mean_fa, mean_fa_mask, mean_fa_skel = tbss_3_postreg(
        tbss_dir=tbss_dir,
        use_fmrib58_fa_mean_and_skel=inputs["use_fmrib58_fa_mean_and_skel"],
        fsl_sh=inputs["fsl_config"])
    outputs["all_FA"] = all_fa
    outputs["mean_FA"] = mean_fa
    outputs["mean_FA_mask"] = mean_fa_mask
    outputs["mean_FA_skel"] = mean_fa_skel


"""
Step 4: stats
"""

if inputs["pre_stats"]:
    print("Project the FA data onto the mean FA skeleton....")
    (all_fa_skeletonized, mean_fa_skel_mask, mean_fa_skel_mask_dst,
     thresh_file) = tbss_4_prestats(
        tbss_dir=tbss_dir,
        threshold=inputs["threshold_fa_skeleton"],
        fsl_sh=inputs["fsl_config"])
    outputs["all_FA_skeletonized"] = all_fa_skeletonized
    outputs["mean_FA_skel_mask"] = mean_fa_skel_mask
    outputs["mean_FA_skel_mask_dst"] = mean_fa_skel_mask_dst
    outputs["mean_FA_skel_mask_dst"] = mean_fa_skel_mask_dst


"""
Dump subjects
"""
subjects_file = os.path.join(tbss_dir, "stats", "subjects.txt")
ordered_subjects = [
    elem.split(".")[0]
    for elem in sorted(glob.glob1(os.path.join(tbss_dir, "FA"), "*_FA.*"))]
with open(subjects_file, "wt") as open_file:
     open_file.write("\n".join(ordered_subjects))


"""
Update the outputs and save them and the inputs in a 'logs' directory.
"""

logdir = os.path.join(inputs["outdir"], "logs")
if not os.path.isdir(logdir):
    os.mkdir(logdir)
for name, final_struct in [("inputs", inputs), ("outputs", outputs),
                           ("runtime", runtime)]:
    log_file = os.path.join(logdir, "{0}.json".format(name))
    with open(log_file, "wt") as open_file:
        json.dump(final_struct, open_file, sort_keys=True, check_circular=True,
                  indent=4)
if verbose > 1:
    print("[final]")
    pprint(outputs)
